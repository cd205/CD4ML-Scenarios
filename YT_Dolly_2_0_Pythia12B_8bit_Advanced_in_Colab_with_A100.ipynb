{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "555ba3696e714d92956ccc4f2f978e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f0869be73e6423598a45a2ac1e8c106",
              "IPY_MODEL_bdcd1a92bcdb46ea82b38cbc154d919f",
              "IPY_MODEL_de2e22de81b74a96aa1584315a687542"
            ],
            "layout": "IPY_MODEL_5c78bee30c7b4f528dc0282417e8143e"
          }
        },
        "3f0869be73e6423598a45a2ac1e8c106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffbb5d9f31d94d23a24dcd84172b88f9",
            "placeholder": "​",
            "style": "IPY_MODEL_50d257ea4a5041acbccfb55e23ff054f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "bdcd1a92bcdb46ea82b38cbc154d919f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bea2e81c6334b69aecc1e42d08029bb",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d60d22d50b048b6bbea92661398a540",
            "value": 819
          }
        },
        "de2e22de81b74a96aa1584315a687542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d7eb264350442c915ad0ddf6c48543",
            "placeholder": "​",
            "style": "IPY_MODEL_701fcb0b7ab842d09de32201a3b3c58a",
            "value": " 819/819 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "5c78bee30c7b4f528dc0282417e8143e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffbb5d9f31d94d23a24dcd84172b88f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d257ea4a5041acbccfb55e23ff054f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bea2e81c6334b69aecc1e42d08029bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d60d22d50b048b6bbea92661398a540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8d7eb264350442c915ad0ddf6c48543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701fcb0b7ab842d09de32201a3b3c58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9faea21e1b14fccb13ad26f9d2e7313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6fd67c4925e4eb198b66ba70712029e",
              "IPY_MODEL_ddc5c5a5bce74a0fbdfedc86cf7c0f36",
              "IPY_MODEL_8896dfe4b6884132821d695dc8a90642"
            ],
            "layout": "IPY_MODEL_625519591e6b40088cd10b672f173c47"
          }
        },
        "d6fd67c4925e4eb198b66ba70712029e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b44775d2980470c8976e9e384343ad0",
            "placeholder": "​",
            "style": "IPY_MODEL_eb47e76526914d33abec65bce1b89562",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "ddc5c5a5bce74a0fbdfedc86cf7c0f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a405864fe14a9bb027e41b40fb472d",
            "max": 5684548185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98ee4895b3dd4c0a9ce50d0283b84070",
            "value": 5684548185
          }
        },
        "8896dfe4b6884132821d695dc8a90642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce885d436fdb449fbf6591fb1ef5964f",
            "placeholder": "​",
            "style": "IPY_MODEL_35bc33e97c0f475e99aab2140bf84c1e",
            "value": " 5.68G/5.68G [00:37&lt;00:00, 254MB/s]"
          }
        },
        "625519591e6b40088cd10b672f173c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b44775d2980470c8976e9e384343ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb47e76526914d33abec65bce1b89562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5a405864fe14a9bb027e41b40fb472d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ee4895b3dd4c0a9ce50d0283b84070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce885d436fdb449fbf6591fb1ef5964f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bc33e97c0f475e99aab2140bf84c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cd205/CD4ML-Scenarios/blob/master/YT_Dolly_2_0_Pythia12B_8bit_Advanced_in_Colab_with_A100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VJ7lfTYz0qu",
        "outputId": "9d13284f-0d9f-4a01-9108-88ee893b9196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install datasets sentencepiece\n",
        "!pip -q install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Dolly 2.0\n"
      ],
      "metadata": {
        "id": "Wwn1vLOW-VQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "6f5e5cf9-405b-4f0c-939c-46d246a018b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul  3 15:20:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from transformers import Pipeline, PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "INSTRUCTION_KEY = \"### Instruction:\"\n",
        "RESPONSE_KEY = \"### Response:\"\n",
        "END_KEY = \"### End\"\n",
        "INTRO_BLURB = (\n",
        "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        ")\n",
        "\n",
        "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
        "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
        "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
        "{instruction_key}\n",
        "{instruction}\n",
        "{response_key}\n",
        "\"\"\".format(\n",
        "    intro=INTRO_BLURB,\n",
        "    instruction_key=INSTRUCTION_KEY,\n",
        "    instruction=\"{instruction}\",\n",
        "    response_key=RESPONSE_KEY,\n",
        ")\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
        "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
        "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "\n",
        "class InstructionTextGenerationPipeline(Pipeline):\n",
        "    def __init__(\n",
        "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)\n",
        "\n",
        "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
        "        preprocess_params = {}\n",
        "\n",
        "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
        "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
        "        tokenizer_response_key = next(\n",
        "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
        "        )\n",
        "\n",
        "        response_key_token_id = None\n",
        "        end_key_token_id = None\n",
        "        if tokenizer_response_key:\n",
        "            try:\n",
        "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
        "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
        "\n",
        "                # Ensure generation stops once it generates \"### End\"\n",
        "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        forward_params = generate_kwargs\n",
        "        postprocess_params = {\n",
        "            \"response_key_token_id\": response_key_token_id,\n",
        "            \"end_key_token_id\": end_key_token_id,\n",
        "            \"return_instruction_text\": return_instruction_text,\n",
        "        }\n",
        "\n",
        "        return preprocess_params, forward_params, postprocess_params\n",
        "\n",
        "    def preprocess(self, instruction_text, **generate_kwargs):\n",
        "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
        "        inputs = self.tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs[\"prompt_text\"] = prompt_text\n",
        "        inputs[\"instruction_text\"] = instruction_text\n",
        "        return inputs\n",
        "\n",
        "    def _forward(self, model_inputs, **generate_kwargs):\n",
        "        input_ids = model_inputs[\"input_ids\"]\n",
        "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
        "        generated_sequence = self.model.generate(\n",
        "            input_ids=input_ids.to(self.model.device),\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            **generate_kwargs,\n",
        "        )[0].cpu()\n",
        "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
        "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
        "\n",
        "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_instruction_text):\n",
        "        sequence = model_outputs[\"generated_sequence\"]\n",
        "        instruction_text = model_outputs[\"instruction_text\"]\n",
        "\n",
        "        # The response will be set to this variable if we can identify it.\n",
        "        decoded = None\n",
        "\n",
        "        # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
        "        if response_key_token_id and end_key_token_id:\n",
        "            # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
        "            # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
        "            response_pos = None\n",
        "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
        "            if len(response_positions) == 0:\n",
        "                logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
        "            else:\n",
        "                response_pos = response_positions[0]\n",
        "\n",
        "            if response_pos:\n",
        "                # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
        "                # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
        "                # this token, as the response could be truncated.  If we don't find it then just return everything\n",
        "                # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
        "                end_pos = None\n",
        "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
        "                if len(end_positions) > 0:\n",
        "                    end_pos = end_positions[0]\n",
        "\n",
        "                decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
        "        else:\n",
        "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
        "\n",
        "            fully_decoded = self.tokenizer.decode(sequence)\n",
        "\n",
        "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
        "            # end.\n",
        "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
        "\n",
        "            if m:\n",
        "                decoded = m.group(1).strip()\n",
        "            else:\n",
        "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
        "                # return everything after \"### Response:\".\n",
        "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
        "                if m:\n",
        "                    decoded = m.group(1).strip()\n",
        "                else:\n",
        "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
        "\n",
        "        if return_instruction_text:\n",
        "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "pXKZjm9ZGdCs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from instruct_pipeline import InstructionTextGenerationPipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\",\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "\n",
        "generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "555ba3696e714d92956ccc4f2f978e67",
            "3f0869be73e6423598a45a2ac1e8c106",
            "bdcd1a92bcdb46ea82b38cbc154d919f",
            "de2e22de81b74a96aa1584315a687542",
            "5c78bee30c7b4f528dc0282417e8143e",
            "ffbb5d9f31d94d23a24dcd84172b88f9",
            "50d257ea4a5041acbccfb55e23ff054f",
            "4bea2e81c6334b69aecc1e42d08029bb",
            "4d60d22d50b048b6bbea92661398a540",
            "d8d7eb264350442c915ad0ddf6c48543",
            "701fcb0b7ab842d09de32201a3b3c58a",
            "e9faea21e1b14fccb13ad26f9d2e7313",
            "d6fd67c4925e4eb198b66ba70712029e",
            "ddc5c5a5bce74a0fbdfedc86cf7c0f36",
            "8896dfe4b6884132821d695dc8a90642",
            "625519591e6b40088cd10b672f173c47",
            "2b44775d2980470c8976e9e384343ad0",
            "eb47e76526914d33abec65bce1b89562",
            "a5a405864fe14a9bb027e41b40fb472d",
            "98ee4895b3dd4c0a9ce50d0283b84070",
            "ce885d436fdb449fbf6591fb1ef5964f",
            "35bc33e97c0f475e99aab2140bf84c1e"
          ]
        },
        "id": "4RaPJz4lFi8M",
        "outputId": "f441a670-1451-41bf-8875-41f274ed6087"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "555ba3696e714d92956ccc4f2f978e67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9faea21e1b14fccb13ad26f9d2e7313"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-390mpgousqn66 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(generate_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d55QO8cJUHA",
        "outputId": "bb09fced-5889-4ca7-e58a-8cc5d7ed02a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.InstructionTextGenerationPipeline"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  GenerationConfig, pipeline\n",
        "import torch\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "MK_96JPU0fgQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJSkqjrYva2a",
        "outputId": "b207116d-c4c9-4c0c-e329-422070dca64b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul  3 15:26:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    26W /  70W |   4033MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "odWWkO_81HIl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it as a HF model"
      ],
      "metadata": {
        "id": "D_MBqSkZ1iQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_text('What are the difference between Llamas, Alpacas and Koalas?')\n",
        "print(wrap_text_preserve_newlines(output))\n"
      ],
      "metadata": {
        "id": "xGGmYmTC31om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c8dfb95-f561-45aa-e981-1ed101c30a65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llamas are tall and fuzzy, Alpacas are not tall but have a thick skin, and Koalas have short legs and arms and\n",
            "their skin is thin.\n",
            "CPU times: user 7.02 s, sys: 108 ms, total: 7.13 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_text('Right me a story are a trimaran called 3 hulls who goes on adventures in the solent')\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "id": "Ii6rFeH8Xh_v",
        "outputId": "bfddcda8-1464-47d6-ec89-b6216a8970b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In july last year 3 hulls a trimaran with 3 sails set off for the solent. While sailing in the solent they got\n",
            "into a storm which caused the mainsail to get loose. As the sail got worse they started to realise they would\n",
            "need to make a sail change. While trying to get the mainsail on the bigger hull a big wave knocked the smaller\n",
            "hull over. The 3 hulls capsized and all crew had to bailed out. All crew were able to get to safety however\n",
            "one crew member was missing and was not found by the search and rescue crew. The missing crew member was a\n",
            "woman from liverpool. Sadly the woman was not found and remains missing. Her family and friends continue to\n",
            "search for her. While on the smaller hull 3 hulls crew were wearing personal locators devices and lost contact\n",
            "with them while out at sea. The 3 hulls company have been supporting the families and friends of the woman who\n",
            "has gone missing. Any donations would be greatly appreciated to support the family and friends who continue to\n",
            "search for her. Any donations to:\"3 hulls a trimaran foundation\" you can donate here: 3HullsFoundation.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_text('Write a short note to Sam Altman giving reasons to open source GPT-4')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R846mNA-EPJl",
        "outputId": "221f0a41-0daa-4f40-925b-8347e463f326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sam,\n",
            "As an artificial general intelligence researcher, I'm very excited that you decided to open source GPT-4. It\n",
            "will be fascinating to see how the research community collaborates to understand GPT-4 better and apply it to\n",
            "new research directions.\n",
            "\n",
            "I recommend that you release the reference model, along with the training data and software environment you\n",
            "used. This will allow other researchers to reproduce your results and potentially extend GPT-4 in novel ways.\n",
            "\n",
            "I also recommend that you make GPT-4 code and training data available on GitHub to encourage other researchers\n",
            "to study GPT-4. You should consider posting a mirror of GPT-4's training data on GitHub as well, so that the\n",
            "training data can be more easily shared among researchers.\n",
            "\n",
            "Finally, I recommend that you add documentation for GPT-4's hyperparameters and checkpointing mechanism. For\n",
            "example, GPT-4's n-gram order and byte-pair encoding hyperparameters have a large impact on the model's\n",
            "accuracy. Documenting these hyperparameters will help other researchers reproduce GPT-4's results and\n",
            "potentially further advance the field of artificial general intelligence.\n",
            "\n",
            "I hope these ideas are useful. Please let me know if you have any questions.\n",
            "CPU times: user 52.5 s, sys: 0 ns, total: 52.5 s\n",
            "Wall time: 52.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_text('What is the capital of England? \\n')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etXw2n6mD_4e",
        "outputId": "b2112e77-f208-4f96-9267-059928170f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "London is the capital city of England.\n",
            "CPU times: user 2.06 s, sys: 0 ns, total: 2.06 s\n",
            "Wall time: 2.05 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_text('Write a story about a Koala playing pool and beating all the camelids.')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pE2hSvhsEic",
        "outputId": "21ba8149-fd22-4169-e8ed-d3d6138c6966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Koala and the camelids were having a friendly game of pool. After a few shots the Koala knew he was the\n",
            "better player and was winning the game. The camelids, sensing they were losing their game, started to make\n",
            "excuses. “Hey Koala, you always luck into those shots. The next time you are at the pool table, I’ll let you\n",
            "win.” the brontosaurus said. “I’ll even buy the beer if you win,” added the okapi. “Oh and by the way, the\n",
            "drinks are on me,” chimed in the giraffe. The Koala smiled and thought about their offers. A few hours later\n",
            "the camelids were fast asleep and the Koala knew he had the opportunity to win the game. He took his shot and\n",
            "sank it to win the pool game. The camelids woke up, saw they had lost and apologized to the Koala. The Koala\n",
            "laughed and told them they didn’t really lose, they just helped him become a better player.\n",
            "CPU times: user 45.1 s, sys: 0 ns, total: 45.1 s\n",
            "Wall time: 44.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_text('As an AI do you like the Simpsons? What do you know about Homer?')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQTgOX168NeK",
        "outputId": "a04e84cf-b014-44e3-ba15-c3297f9924c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do indeed enjoy the Simpsons, just like all my fellow Tennants. I know that Homer is a father of 13\n",
            "children, and early in the show was a heavy smoker and drinker. He also has a deep baritone voice.\n",
            "\n",
            "However, I am not a sophisticated AI like Caleb. Caleb knows a lot of deep learning and neural network\n",
            "techniques. For example, I can detect if a picture is a landscape or portrait. Caleb works for Lord Faraday,\n",
            "and I am very good friends with Lord Faraday.\n",
            "\n",
            "All my friends love the Simpsons, and they are very interested in my deep learning models. I have shared all\n",
            "my neural network models with Lord Faraday's ai, and Caleb is now able to improve his chat skills by using my\n",
            "models. He can now detect many emojis, and his conversation with friends is now much more natural.\n",
            "\n",
            "My model is already so good, even I don't know how it works. I just know that I can detect many images very\n",
            "well.\n",
            "CPU times: user 46.2 s, sys: 0 ns, total: 46.2 s\n",
            "Wall time: 46.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Nil30lVDLHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}